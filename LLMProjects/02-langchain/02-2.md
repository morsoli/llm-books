æœ¬ç”µå­ä¹¦å¼€æºï¼Œæ¬¢è¿ star ğŸŒŸï¼Œå…³æ³¨ã€ŠLLM åº”ç”¨å¼€å‘å®è·µç¬”è®°ã€‹

æˆ‘çš„æ–°ä¹¦[ã€ŠLangChainç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µã€‹](https://u.jd.com/V8pkqFY) å·²ç»å¼€å”®ï¼æ¨èæ­£åœ¨å­¦ä¹ AIåº”ç”¨å¼€å‘çš„æœ‹å‹è´­ä¹°é˜…è¯»ï¼
[![LangChainç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ](../../images/langchain-book.jpg "LangChainç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ")](https://u.jd.com/V8pkqFY) 

## LangChainæ¨¡å—è§£è¯»
LangChain åˆ†ä¸º 6 ä¸ªæ¨¡å—ï¼Œåˆ†åˆ«æ˜¯å¯¹ï¼ˆå¤§è¯­è¨€ï¼‰æ¨¡å‹è¾“å…¥è¾“å‡ºçš„ç®¡ç†ã€å¤–éƒ¨æ•°æ®æ¥å…¥ã€[é“¾çš„æ¦‚å¿µ](./02-2-1.md)ã€ï¼ˆä¸Šä¸‹æ–‡è®°å¿†ï¼‰å­˜å‚¨ç®¡ç†ã€æ™ºèƒ½ä»£ç†ä»¥åŠå›è°ƒç³»ç»Ÿï¼Œé€šè¿‡æ–‡æ¡£çš„ç»„ç»‡ç»“æ„ï¼Œä½ å¯ä»¥æ¸…æ™°äº†è§£åˆ° LangChainçš„ä¾§é‡ç‚¹ï¼Œåœ¨å¤§è¯­è¨€æ¨¡å‹å¼€å‘ç”Ÿæ€ä¸­å¯¹è‡ªå·±çš„å®šä½ã€‚ä¸‹é¢æˆ‘å°†å¯¹å„ä¸ªæ¨¡å‹é€ä¸ªè¿›è¡Œè§£è¯»ã€‚


### [Model I/O](https://python.langchain.com/docs/modules/model_io/)
è¿™éƒ¨åˆ†åŒ…æ‹¬å¯¹å¤§è¯­è¨€æ¨¡å‹è¾“å…¥è¾“å‡ºçš„ç®¡ç†ï¼Œè¾“å…¥ç¯èŠ‚çš„æç¤ºè¯ç®¡ç†ï¼ˆåŒ…å«æ¨¡æ¿åŒ–æç¤ºè¯å’Œæç¤ºè¯åŠ¨æ€é€‰æ‹©ç­‰ï¼‰ï¼Œå¤„ç†ç¯èŠ‚çš„è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬æ‰€æœ‰LLMsçš„é€šç”¨æ¥å£ï¼Œä»¥åŠå¸¸ç”¨çš„LLMså·¥å…·ï¼›Chatæ¨¡å‹æ˜¯ä¸€ç§ä¸LLMsä¸åŒçš„APIï¼Œç”¨æ¥å¤„ç†æ¶ˆæ¯ï¼‰ï¼Œè¾“å‡ºç¯èŠ‚åŒ…æ‹¬ä»æ¨¡å‹è¾“å‡ºä¸­æå–ä¿¡æ¯ã€‚

![](https://python.langchain.com/assets/images/model_io-1f23a36233d7731e93576d6885da2750.jpg)

#### æç¤ºè¯ç®¡ç†
- **æç¤ºæ¨¡æ¿**
åŠ¨æ€æç¤ºè¯=æç¤ºæ¨¡æ¿+å˜é‡ï¼Œé€šè¿‡å¼•å…¥ç»™æç¤ºè¯å¼•å…¥å˜é‡çš„æ–¹å¼ï¼Œä¸€æ–¹é¢ä¿è¯äº†çµæ´»æ€§ï¼Œä¸€æ–¹é¢åˆèƒ½ä¿è¯Promptå†…å®¹ç»“æ„è¾¾åˆ°æœ€ä½³

    ```python
    from langchain import PromptTemplate
    no_input_prompt = PromptTemplate(input_variables=[], template="Tell me a joke.")
    no_input_prompt.format()

    one_input_prompt = PromptTemplate(input_variables=["adjective"], template="Tell me a {adjective} joke.")
    # "Tell me a funny chickens."
    one_input_prompt.format(adjective="funny")

    multiple_input_prompt = PromptTemplate(
        input_variables=["adjective", "content"], 
        template="Tell me a {adjective} joke about {content}."
    )
    # "Tell me a funny joke about chickens."
    multiple_input_prompt.format(adjective="funny", content="chickens")
    ```

- **èŠå¤©æç¤ºæ¨¡æ¿**
èŠå¤©åœºæ™¯ä¸­ï¼Œæ¶ˆæ¯å¯ä»¥ä¸AIã€äººç±»æˆ–ç³»ç»Ÿè§’è‰²ç›¸å…³è”ï¼Œæ¨¡å‹åº”è¯¥æ›´åŠ å¯†åˆ‡åœ°éµå¾ªç³»ç»ŸèŠå¤©æ¶ˆæ¯çš„æŒ‡ç¤ºã€‚è¿™ä¸ªæ˜¯å¯¹ OpenAI gpt-3.5-tubor APIä¸­roleå­—æ®µï¼ˆrole çš„å±æ€§ç”¨äºæ˜¾å¼å®šä¹‰è§’è‰²ï¼Œå…¶ä¸­ system ç”¨äºç³»ç»Ÿé¢„è®¾ï¼Œæ¯”å¦‚â€ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘å®¶â€œï¼Œâ€œä½ æ˜¯ä¸€ä¸ªå†™ä½œåŠ©æ‰‹â€ï¼Œuser è¡¨ç¤ºç”¨æˆ·çš„è¾“å…¥ï¼Œ assistant è¡¨ç¤ºæ¨¡å‹çš„è¾“å‡ºï¼‰çš„ä¸€ç§æŠ½è±¡ï¼Œä»¥ä¾¿åº”ç”¨äºå…¶ä»–å¤§è¯­è¨€æ¨¡å‹ã€‚SystemMessageå¯¹åº”ç³»ç»Ÿé¢„è®¾ï¼ŒHumanMessageç”¨æˆ·è¾“å…¥ï¼ŒAIMessageè¡¨ç¤ºæ¨¡å‹è¾“å‡ºï¼Œä½¿ç”¨ ChatMessagePromptTemplate å¯ä»¥ä½¿ç”¨ä»»æ„è§’è‰²æ¥æ”¶èŠå¤©æ¶ˆæ¯ã€‚

    ```python
    from langchain.prompts import (
        ChatPromptTemplate,
        PromptTemplate,
        SystemMessagePromptTemplate,
        AIMessagePromptTemplate,
        HumanMessagePromptTemplate,
    )
    from langchain.schema import (
        AIMessage,
        HumanMessage,
        SystemMessage
    )

    def generate_template():
        template="You are a helpful assistant that translates {input_language} to {output_language}."
        system_message_prompt = SystemMessagePromptTemplate.from_template(template)
        human_template="{text}"
        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
        
        chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])
        # [SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}), HumanMessage(content='I like Large Language Model', additional_kwargs={}, example=False)]
        final_message = chat_prompt.format_prompt(input_language="English", output_language="Chinese", text="I like Large Language Model").to_messages()
        print(final_message)

    if __name__ == "__main__":
        generate_template()
    ```

- **å…¶ä»–**
  - åŸºäº StringPromptTemplate è‡ªå®šä¹‰æç¤ºæ¨¡æ¿StringPromptTemplate
  - å°†Promptè¾“å…¥ä¸ç‰¹å¾å­˜å‚¨å…³è”èµ·æ¥(FeaturePromptTemplate)
  - å°‘æ ·æœ¬æç¤ºæ¨¡æ¿ï¼ˆFewShotPromptTemplateï¼‰
  - ä»ç¤ºä¾‹ä¸­åŠ¨æ€æå–æç¤ºè¯âœï¸

#### LLMs
- **LLMs**

    å°†æ–‡æœ¬å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥å¹¶è¿”å›æ–‡æœ¬å­—ç¬¦ä¸²çš„æ¨¡å‹ï¼ˆçº¯æ–‡æœ¬è¡¥å…¨æ¨¡å‹ï¼‰ï¼Œè¿™é‡Œé‡ç‚¹è¯´ä¸‹åšé¡¹ç›®å°½é‡ç”¨å¼‚æ­¥çš„æ–¹å¼ï¼Œä½“éªŒä¼šæ›´å¥½ï¼Œä¸‹é¢çš„ä¾‹å­è¿ç»­10ä¸ªè¯·æ±‚ï¼Œæ—¶é—´ç›¸å·®æ¥è¿‘5sã€‚
    ```python
    import time
    import asyncio
    from langchain.llms import OpenAI
    def generate_serially():
        llm = OpenAI(temperature=0.9)
        for _ in range(10):
            resp = llm.generate(["Hello, how are you?"])
            print(resp.generations[0][0].text)

    async def async_generate(llm):
        resp = await llm.agenerate(["Hello, how are you?"])
        print(resp.generations[0][0].text)

    async def generate_concurrently():
        llm = OpenAI(temperature=0.9)
        tasks = [async_generate(llm) for _ in range(10)]
        await asyncio.gather(*tasks)

    if __name__ == "__main__":
        s = time.perf_counter()
        asyncio.run(generate_concurrently())
        elapsed = time.perf_counter() - s
        print("\033[1m" + f"Concurrent executed in {elapsed:0.2f} seconds." + "\033[0m")

        s = time.perf_counter()
        generate_serially()
        elapsed = time.perf_counter() - s
        print("\033[1m" + f"Serial executed in {elapsed:0.2f} seconds." + "\033[0m")
    ```

- **ç¼“å­˜**

  å¦‚æœå¤šæ¬¡è¯·æ±‚çš„è¿”å›ä¸€æ ·ï¼Œå°±å¯ä»¥è€ƒè™‘ä½¿ç”¨ç¼“å­˜ï¼Œä¸€æ–¹é¢å¯ä»¥å‡å°‘å¯¹APIè°ƒç”¨æ¬¡æ•°èŠ‚çœtokenæ¶ˆè€—ï¼Œä¸€æ–¹é¢å¯ä»¥åŠ å¿«åº”ç”¨ç¨‹åºçš„é€Ÿåº¦ã€‚
    ```python
    from langchain.cache import InMemoryCache
    import time
    import langchain
    from langchain.llms import OpenAI
    llm = OpenAI(model_name="text-davinci-002", n=2, best_of=2)
    langchain.llm_cache = InMemoryCache()
    s = time.perf_counter()
    llm("Tell me a joke")
    elapsed = time.perf_counter() - s
    # executed first in 2.18 seconds.
    print("\033[1m" + f"executed first in {elapsed:0.2f} seconds." + "\033[0m")
    llm("Tell me a joke")
    # executed second in 0.72 seconds.
    elapsed2 = time.perf_counter() - elapsed
    print("\033[1m" + f"executed second in {elapsed2:0.2f} seconds." + "\033[0m")
    ```

- **æµå¼ä¼ è¾“**

  ä»¥æ‰“å­—æœºæ•ˆæœçš„æ–¹å¼é€å­—è¿”å›èŠå¤©å†…å®¹

    ```python
    from langchain.llms import OpenAI
    from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
    llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)
    resp = llm("æ¨¡ä»¿æç™½çš„é£æ ¼å†™ä¸€é¦–å”è¯—.")
    print(resp)
    ```
- **è·Ÿè¸ª token æ¶ˆè€—æƒ…å†µ**
    æµå¼ä¼ è¾“çš„æƒ…å†µä¸‹æš‚ä¸æ”¯æŒè®¡ç®—ï¼Œå¯ä»¥è€ƒè™‘å†…å®¹å…¨éƒ¨ä¼ è¾“å®Œæˆåç”¨tiktokenåº“è®¡ç®—
    ```python
    from langchain.llms import OpenAI
    from langchain.callbacks import get_openai_callback
    llm = OpenAI()
    with get_openai_callback() as cb:
        resp = llm.generate(["æ¨¡ä»¿æç™½çš„é£æ ¼å†™ä¸€é¦–å”è¯—."])
        print(resp.generations[0][0].text)
        print(cb)
    ```

- **Chat models**
å°†èŠå¤©æ¶ˆæ¯åˆ—è¡¨ä½œä¸ºè¾“å…¥å¹¶è¿”å›èŠå¤©æ¶ˆæ¯çš„æ¨¡å‹ï¼ˆå¯¹è¯è¡¥å…¨æ¨¡å‹ï¼‰

- **å…¶ä»–**
  - ä»¥jsonæˆ–è€…ymlæ ¼å¼è¯»å–ä¿å­˜LLMçš„ï¼ˆå‚æ•°ï¼‰é…ç½®ï¼ˆllm.load_llmæ–¹æ³•å’Œllm.saveæ–¹æ³•ï¼‰
  - ä¸ºäº†èŠ‚çœä½ çš„tokenï¼Œè¿˜å¯ä»¥åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ä½¿ç”¨ä¸€ä¸ªæ¨¡æ‹ŸLLMè¾“å‡ºçš„`FakeListLLM`ï¼›è¿˜æœ‰ä¸€ä¸ªæ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥çš„`HumanInputLLM`ã€‚
  - ä¸å…¶ä»– AIç›¸å…³åŸºç¡€è®¾æ–½çš„[é›†æˆ](https://python.langchain.com/docs/modules/model_io/models/llms/integrations/ai21)ï¼Œç”¨åˆ°éšæ—¶æŸ¥è¯¢å³å¯

#### è¾“å‡ºè§£æå™¨

è¾“å‡ºè§£æå™¨ç”¨äºæ„é€ å¤§è¯­è¨€æ¨¡å‹çš„å“åº”æ ¼å¼ï¼Œå…·ä½“é€šè¿‡æ ¼å¼åŒ–æŒ‡ä»¤å’Œè‡ªå®šä¹‰æ–¹æ³•ä¸¤ç§æ–¹å¼ã€‚
```python
# æ ¼å¼åŒ–æŒ‡ä»¤çš„æ–¹å¼
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
output_parser = CommaSeparatedListOutputParser()
format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="åˆ—å‡ºäº”ä¸ª {subject}.\n{format_instructions}",
    input_variables=["subject"],
    partial_variables={"format_instructions": format_instructions}
)
model = OpenAI(temperature=0)
_input = prompt.format(subject="å¤§è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§")
output = model(_input)
# å¯ç§»æ¤æ€§, å¯æ‰©å±•æ€§, å¯é‡ç”¨æ€§, å¯ç»´æŠ¤æ€§, å¯è¯»æ€§
print(output)
output_parser.parse(output)
```

è™½ç„¶å†…ç½®äº† DatetimeOutputParserã€EnumOutputParserã€PydanticOutputParserç­‰è§£æå™¨ï¼Œä½†æ˜¯æˆ‘è§‰å¾—ResponseSchemaçš„æ§åˆ¶è‡ªç”±åº¦æ›´å¥½ï¼Œä½†æ˜¯ä¸æ˜“äºç®¡ç†ã€‚
```python
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
response_schemas = [
    ResponseSchema(name="answer", description="answer to the user's question"),
    ResponseSchema(name="source", description="source used to answer the user's question, should be a website.")
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="answer the users question as best as possible.\n{format_instructions}\n{question}",
    input_variables=["question"],
    partial_variables={"format_instructions": format_instructions}
)
```

### [Data Connection](https://python.langchain.com/docs/modules/data_connection/)
æ‰“é€šå¤–éƒ¨æ•°æ®çš„ç®¡é“ï¼ŒåŒ…å«æ–‡æ¡£åŠ è½½ï¼Œæ–‡æ¡£è½¬æ¢ï¼Œæ–‡æœ¬åµŒå…¥ï¼Œå‘é‡å­˜å‚¨å‡ ä¸ªç¯èŠ‚ã€‚
![](https://python.langchain.com/assets/images/data_connection-c42d68c3d092b85f50d08d4cc171fc25.jpg)

#### [æ–‡æ¡£åŠ è½½](https://python.langchain.com/docs/modules/data_connection/document_loaders/)
é‡ç‚¹åŒ…æ‹¬äº†csvï¼ˆCSVLoaderï¼‰ï¼Œhtmlï¼ˆUnstructuredHTMLLoaderï¼‰ï¼Œjsonï¼ˆJSONLoaderï¼‰ï¼Œmarkdownï¼ˆUnstructuredMarkdownLoaderï¼‰ä»¥åŠpdfï¼ˆå› ä¸ºpdfçš„æ ¼å¼æ¯”è¾ƒå¤æ‚ï¼Œæä¾›äº†PyPDFLoaderã€MathpixPDFLoaderã€UnstructuredPDFLoaderï¼ŒPyMuPDFç­‰å¤šç§å½¢å¼çš„åŠ è½½å¼•æ“ï¼‰å‡ ç§å¸¸ç”¨æ ¼å¼çš„å†…å®¹è§£æï¼Œä½†æ˜¯åœ¨å®é™…çš„é¡¹ç›®ä¸­ï¼Œæ•°æ®æ¥æºä¸€èˆ¬æ¯”è¾ƒå¤šæ ·ï¼Œæ ¼å¼ä¹Ÿæ¯”è¾ƒå¤æ‚ï¼Œé‡ç‚¹æ¨èæŒ‰éœ€å»æŸ¥çœ‹ä¸å„ç§[æ•°æ®æº
é›†æˆ](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/acreom)çš„ç« èŠ‚è¯´æ˜ï¼ŒDiscordã€Notionã€Joplinï¼ŒWordç­‰æ•°æ®æºã€‚

#### [æ–‡æ¡£æ‹†åˆ†](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
é‡ç‚¹å…³æ³¨æŒ‰ç…§å­—ç¬¦é€’å½’æ‹†åˆ†çš„æ–¹å¼ RecursiveCharacterTextSplitter ï¼Œè¿™ç§æ–¹å¼ä¼šå°†è¯­ä¹‰æœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µæ”¾åœ¨ä¸€èµ·ã€‚

#### [æ–‡æœ¬åµŒå…¥](https://python.langchain.com/docs/modules/data_connection/text_embedding/)
åµŒå…¥åŒ…å«ä¸¤ä¸ªæ–¹æ³•ï¼Œä¸€ä¸ªç”¨äºåµŒå…¥æ–‡æ¡£ï¼Œæ¥å—å¤šä¸ªæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼›ä¸€ä¸ªç”¨äºåµŒå…¥æŸ¥è¯¢ï¼Œæ¥å—å•ä¸ªæ–‡æœ¬ã€‚æ–‡æ¡£ä¸­ç¤ºä¾‹ä½¿ç”¨äº†OpenAIçš„åµŒå…¥æ¨¡å‹text-embedding-ada-002ï¼Œä½†æä¾›äº†å¾ˆå¤šç¬¬ä¸‰æ–¹[åµŒå…¥æ¨¡å‹](https://python.langchain.com/docs/modules/data_connection/text_embedding/integrations/aleph_alpha)é›†æˆå¯ä»¥æŒ‰éœ€æŸ¥çœ‹ã€‚
```python
from langchain.embeddings import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings()
# åµŒå…¥æ–‡æœ¬
embeddings = embedding_model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!"
    ]
)
len(embeddings), len(embeddings[0])
# åµŒå…¥æŸ¥è¯¢
embedded_query = embedding_model.embed_query("What was the name mentioned in the conversation?")
embedded_query[:5]
```

#### [å‘é‡å­˜å‚¨](https://python.langchain.com/docs/modules/data_connection/vectorstores/)
è¿™ä¸ªå°±æ˜¯å¯¹å¸¸ç”¨çŸ¢é‡æ•°æ®åº“ï¼ˆFAISSï¼ŒMilvusï¼ŒPineconeï¼ŒPGVectorç­‰ï¼‰å°è£…æ¥å£çš„è¯´æ˜ï¼Œè¯¦ç»†çš„å¯ä»¥å‰å¾€[åµŒå…¥ä¸“é¢˜](./02-3.md)æŸ¥çœ‹ã€‚å¤§æ¦‚æµç¨‹éƒ½ä¸€æ ·ï¼šåˆå§‹åŒ–æ•°æ®åº“è¿æ¥ä¿¡æ¯â€”â€”>å»ºç«‹ç´¢å¼•â€”â€”>å­˜å‚¨çŸ¢é‡â€”â€”>ç›¸ä¼¼æ€§æŸ¥è¯¢ï¼Œä¸‹é¢ä»¥ Pineconeä¸ºä¾‹ï¼š
```python
from langchain.document_loaders import TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings
import pinecone
loader = TextLoader("../../../state_of_the_union.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.split_documents(documents)

embeddings = OpenAIEmbeddings()

pinecone.init(
    api_key=PINECONE_API_KEY, 
    environment=PINECONE_ENV,
)
index_name = "langchain-demo"
docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)
query = "What did the president say about Ketanji Brown Jackson"
docs = docsearch.similarity_search(query)
```

#### [æ•°æ®æŸ¥è¯¢](https://python.langchain.com/docs/modules/data_connection/retrievers/)
è¿™èŠ‚é‡ç‚¹å…³æ³¨æ•°æ®å‹ç¼©ï¼Œç›®çš„æ˜¯è·å¾—ç›¸å…³æ€§æœ€é«˜çš„æ–‡æœ¬å¸¦å…¥promptä¸Šä¸‹æ–‡ï¼Œè¿™æ ·æ—¢å¯ä»¥å‡å°‘tokenæ¶ˆè€—ï¼Œä¹Ÿå¯ä»¥ä¿è¯LLMçš„è¾“å‡ºè´¨é‡ã€‚
```python
from langchain.llms import OpenAI
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.document_loaders import TextLoader
from langchain.vectorstores import FAISS

documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()
docs = retriever.get_relevant_documents("What did the president say about Ketanji Brown Jackson")
# åŸºç¡€æ£€ç´¢ä¼šè¿”å›ä¸€ä¸ªæˆ–ä¸¤ä¸ªç›¸å…³çš„æ–‡æ¡£å’Œä¸€äº›ä¸ç›¸å…³çš„æ–‡æ¡£ï¼Œå³ä½¿æ˜¯ç›¸å…³çš„æ–‡æ¡£ä¹Ÿæœ‰å¾ˆå¤šä¸ç›¸å…³çš„ä¿¡æ¯
pretty_print_docs(docs)

llm = OpenAI(temperature=0)
compressor = LLMChainExtractor.from_llm(llm)
# è¿­ä»£å¤„ç†æœ€åˆè¿”å›çš„æ–‡æ¡£ï¼Œå¹¶ä»æ¯ä¸ªæ–‡æ¡£ä¸­åªæå–ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹
compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)

compressed_docs = compression_retriever.get_relevant_documents("What did the president say about Ketanji Jackson Brown")
pretty_print_docs(compressed_docs)
```

é’ˆå¯¹åŸºç¡€æ£€ç´¢å¾—åˆ°çš„æ–‡æ¡£å†åšä¸€æ¬¡å‘é‡ç›¸ä¼¼æ€§æœç´¢è¿›è¡Œè¿‡æ»¤ï¼Œä¹Ÿå¯ä»¥å–å¾—ä¸é”™çš„æ•ˆæœã€‚
```python
from langchain.retrievers.document_compressors import EmbeddingsFilter

embeddings = OpenAIEmbeddings()
embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever)
```

æœ€åä¸€ç‚¹å°±æ˜¯è‡ªæŸ¥è¯¢ï¼ˆSelfQueryRetrieverï¼‰çš„æ¦‚å¿µï¼Œå…¶å®å°±æ˜¯ç»“æ„åŒ–æŸ¥è¯¢å…ƒæ•°æ®ï¼Œå› ä¸ºå¯¹æ–‡æ¡£çš„å…ƒä¿¡æ¯æŸ¥è¯¢å’Œæ–‡æ¡£å†…å®¹çš„æ¦‚è¦æè¿°éƒ¨åˆ†æŸ¥è¯¢æ•ˆç‡è‚¯å®šæ˜¯é«˜äºå…¨éƒ¨æ–‡æ¡£çš„ã€‚


### [Memory](https://python.langchain.com/docs/modules/memory/)
Chainå’ŒAgentæ˜¯æ— çŠ¶æ€çš„ï¼Œåªèƒ½ç‹¬ç«‹åœ°å¤„ç†æ¯ä¸ªä¼ å…¥çš„æŸ¥è¯¢ï¼ŒMemory å¯ä»¥ç®¡ç†å’Œæ“ä½œå†å²æ¶ˆæ¯ã€‚ä¸€ä¸ªå¸¦å­˜å‚¨çš„Agentä¾‹å­å¦‚ä¸‹ï¼š
```python
def test_memory():
    search = GoogleSearchAPIWrapper()
    tools = [
        Tool(
            name="Search",
            func=search.run,
            description="useful for when you need to answer questions about current events",
        )
    ]
    prefix = """Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:"""
    suffix = """Begin!"

    {chat_history}
    Question: {input}
    {agent_scratchpad}"""

    prompt = ZeroShotAgent.create_prompt(
        tools,
        prefix=prefix,
        suffix=suffix,
        input_variables=["input", "chat_history", "agent_scratchpad"],
    )
    memory = ConversationBufferMemory(memory_key="chat_history")
    llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)
    agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)
    agent_chain = AgentExecutor.from_agent_and_tools(
        agent=agent, tools=tools, verbose=True, memory=memory
    )
    print(agent_chain.run(input="ä¸­å›½æœ‰å¤šå°‘äººå£?"))

```

### èµ„æºæ¨è
1. [Guidance](https://github.com/microsoft/guidance)ï¼šå¾®è½¯å¼€æº prompt ç¼–ç¨‹æ¡†æ¶
2. [LangGPT](https://github.com/yzfly/LangGPT)ï¼šä¸€ç§é¢å‘å¤§æ¨¡å‹çš„ prompt ç¼–ç¨‹è¯­è¨€


