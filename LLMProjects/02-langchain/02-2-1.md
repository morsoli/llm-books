æœ¬ç”µå­ä¹¦å¼€æºï¼Œæ¬¢è¿ star ğŸŒŸï¼Œå…³æ³¨ã€ŠLLM åº”ç”¨å¼€å‘å®è·µç¬”è®°ã€‹

> **äº¤æµç¾¤** åˆ›å»ºäº†ä¸€ä¸ªLLMåº”ç”¨å¼€å‘äº¤æµç¾¤ï¼Œæœ‰éœ€è¦çš„å¯ä»¥é€‰æ‹©åŠ å…¥
![](../images/group.png)

## LangChainæ¨¡å—ä¹‹ [Chains](https://python.langchain.com/docs/modules/chains/)


é“¾å®šä¹‰ä¸ºå¯¹ç»„ä»¶çš„ä¸€ç³»åˆ—è°ƒç”¨ï¼Œä¹Ÿå¯ä»¥åŒ…æ‹¬å…¶ä»–é“¾ï¼Œè¿™ç§åœ¨é“¾ä¸­å°†ç»„ä»¶ç»„åˆåœ¨ä¸€èµ·çš„æƒ³æ³•å¾ˆç®€å•ä½†åŠŸèƒ½å¼ºå¤§ï¼Œæå¤§åœ°ç®€åŒ–äº†å¤æ‚åº”ç”¨ç¨‹åºçš„å®ç°å¹¶ä½¿å…¶æ›´åŠ æ¨¡å—åŒ–ï¼Œè¿™åè¿‡æ¥åˆä½¿è°ƒè¯•ã€ç»´æŠ¤å’Œæ”¹è¿›åº”ç”¨ç¨‹åºå˜å¾—æ›´åŠ å®¹æ˜“ã€‚
ChainåŸºç±»æ˜¯æ‰€æœ‰chainå¯¹è±¡çš„åŸºæœ¬å…¥å£ï¼Œä¸ç”¨æˆ·ç¨‹åºäº¤äº’ï¼Œå¤„ç†ç”¨æˆ·çš„è¾“å…¥ï¼Œå‡†å¤‡å…¶ä»–æ¨¡å—çš„è¾“å…¥ï¼Œæä¾›å†…å­˜èƒ½åŠ›ï¼Œchainçš„å›è°ƒèƒ½åŠ›ï¼Œå…¶ä»–æ‰€æœ‰çš„ Chain ç±»éƒ½ç»§æ‰¿è‡ªè¿™ä¸ªåŸºç±»ï¼Œå¹¶æ ¹æ®éœ€è¦å®ç°ç‰¹å®šçš„åŠŸèƒ½ã€‚

```python
class Chain(BaseModel, ABC):
    memory: BaseMemory
    callbacks: Callbacks

    def __call__(
        self,
        inputs: Any,
        return_only_outputs: bool = False,
        callbacks: Callbacks = None,
    ) -> Dict[str, Any]:
        ...
```

**å®ç°è‡ªå®šä¹‰é“¾**
```python
from __future__ import annotations

from typing import Any, Dict, List, Optional

from pydantic import Extra

from langchain.base_language import BaseLanguageModel
from langchain.callbacks.manager import (
    AsyncCallbackManagerForChainRun,
    CallbackManagerForChainRun,
)
from langchain.chains.base import Chain
from langchain.prompts.base import BasePromptTemplate


class MyCustomChain(Chain):
    prompt: BasePromptTemplate
    llm: BaseLanguageModel
    output_key: str = "text"

    class Config:
        extra = Extra.forbid
        arbitrary_types_allowed = True

    @property
    def input_keys(self) -> List[str]:
        """pomptä¸­çš„åŠ¨æ€å˜é‡
        """
        return self.prompt.input_variables

    @property
    def output_keys(self) -> List[str]:
        """å…è®¸ç›´æ¥è¾“å‡ºçš„åŠ¨æ€å˜é‡.
        """
        return [self.output_key]
    # åŒæ­¥è°ƒç”¨
    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, str]:
        # ä¸‹é¢æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰é€»è¾‘å®ç°
        prompt_value = self.prompt.format_prompt(**inputs)
        # è°ƒç”¨ä¸€ä¸ªè¯­è¨€æ¨¡å‹æˆ–å¦ä¸€ä¸ªé“¾æ—¶ï¼Œä¼ é€’ä¸€ä¸ªå›è°ƒå¤„ç†ã€‚è¿™æ ·å†…éƒ¨è¿è¡Œå¯ä»¥é€šè¿‡è¿™ä¸ªå›è°ƒï¼ˆè¿›è¡Œé€»è¾‘å¤„ç†ï¼‰ã€‚
        response = self.llm.generate_prompt(
            [prompt_value], callbacks=run_manager.get_child() if run_manager else None
        )
        # å›è°ƒå‡ºå‘æ—¶çš„æ—¥å¿—è¾“å‡º
        if run_manager:
            run_manager.on_text("Log something about this run")

        return {self.output_key: response.generations[0][0].text}
    # å¼‚æ­¥è°ƒç”¨
    async def _acall(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,
    ) -> Dict[str, str]:
        prompt_value = self.prompt.format_prompt(**inputs)
        response = await self.llm.agenerate_prompt(
            [prompt_value], callbacks=run_manager.get_child() if run_manager else None
        )
        if run_manager:
            await run_manager.on_text("Log something about this run")

        return {self.output_key: response.generations[0][0].text}

    @property
    def _chain_type(self) -> str:
        return "my_custom_chain"
```


ç»§æ‰¿Chainçš„å­ç±»ä¸»è¦æœ‰ä¸¤ç§ç±»å‹ï¼š

**é€šç”¨å·¥å…· chain**: æ§åˆ¶chainçš„è°ƒç”¨é¡ºåºï¼Œ æ˜¯å¦è°ƒç”¨ï¼Œä»–ä»¬å¯ä»¥ç”¨æ¥åˆå¹¶æ„é€ å…¶ä»–çš„chainã€‚
**ä¸“é—¨ç”¨é€” chain**: å’Œé€šç”¨chainæ¯”è¾ƒæ¥è¯´ï¼Œä»–ä»¬æ‰¿æ‹…äº†å…·ä½“çš„æŸé¡¹ä»»åŠ¡ï¼Œå¯ä»¥å’Œé€šç”¨çš„chainç»„åˆèµ·æ¥ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚æœ‰äº› Chain ç±»å¯èƒ½ç”¨äºå¤„ç†æ–‡æœ¬æ•°æ®ï¼Œæœ‰äº›å¯èƒ½ç”¨äºå¤„ç†å›¾åƒæ•°æ®ï¼Œæœ‰äº›å¯èƒ½ç”¨äºå¤„ç†éŸ³é¢‘æ•°æ®ç­‰ã€‚

#### ä» LangChainHub åŠ è½½é“¾
[LangChainHub](https://github.com/hwchase17/langchain-hub) æ‰˜ç®¡äº†ä¸€äº›é«˜è´¨é‡Promptã€Agentå’ŒChainï¼Œå¯ä»¥ç›´æ¥åœ¨langchainä¸­ä½¿ç”¨ã€‚

```python
def test_mathchain():
    from langchain.chains import load_chain
    chain = load_chain("lc://chains/llm-math/chain.json")
    """
    > Entering new  chain...
    2+2ç­‰äºå‡ Answer: 4
    > Finished chain.
    Answer: 4
    """
    print(chain.run("2+2ç­‰äºå‡ "))
```

#### è¿è¡Œ LLM é“¾çš„äº”ç§æ–¹å¼
```python
from langchain import PromptTemplate, OpenAI, LLMChain

prompt_template = "ç»™åš {product} çš„å…¬å¸èµ·ä¸€ä¸ªåå­—?"

llm = OpenAI(temperature=0)
llm_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate.from_template(prompt_template)
)
print(llm_chain("å„¿ç«¥ç©å…·"))
print(llm_chain.run("å„¿ç«¥ç©å…·"))
llm_chain.apply([{"product":"å„¿ç«¥ç©å…·"}])
llm_chain.generate([{"product":"å„¿ç«¥ç©å…·"}])
llm_chain.predict(product="å„¿ç«¥ç©å…·")
```

### é€šç”¨å·¥å…·chain
1. MultiPromptChainï¼šå¯ä»¥åŠ¨æ€é€‰æ‹©ä¸ç»™å®šé—®é¢˜æœ€ç›¸å…³çš„æç¤ºï¼Œç„¶åä½¿ç”¨è¯¥æç¤ºå›ç­”é—®é¢˜ã€‚
2. EmbeddingRouterChainï¼šä½¿ç”¨åµŒå…¥å’Œç›¸ä¼¼æ€§åŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªé“¾ã€‚
3. LLMRouterChainï¼šä½¿ç”¨ LLM æ¥ç¡®å®šåŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªé“¾ã€‚
4. SimpleSequentialChain/SequentialChainï¼šå°†å¤šä¸ªé“¾æŒ‰ç…§é¡ºåºç»„æˆå¤„ç†æµæ°´çº¿ï¼ŒSimpleMemoryæ”¯æŒåœ¨å¤šä¸ªé“¾ä¹‹é—´ä¼ é€’ä¸Šä¸‹æ–‡
5. TransformChainï¼šä¸€ä¸ªè‡ªå®šä¹‰æ–¹æ³•åšåŠ¨æ€è½¬æ¢çš„é“¾
    ```python
    def transform_func(inputs: dict) -> dict:
        text = inputs["text"]
        shortened_text = "\n\n".join(text.split("\n\n")[:3])
        return {"output_text": shortened_text}


    transform_chain = TransformChain(
        input_variables=["text"], output_variables=["output_text"], transform=transform_func
    )
    template = """Summarize this text:

    {output_text}

    Summary:"""
    prompt = PromptTemplate(input_variables=["output_text"], template=template)
    llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)
    sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])
    ```

### åˆå¹¶æ–‡æ¡£çš„é“¾ï¼ˆä¸“é—¨ç”¨é€”chainï¼‰
BaseCombineDocumentsChain æœ‰å››ç§ä¸åŒçš„æ¨¡å¼
```python
def load_qa_chain(
    llm: BaseLanguageModel,
    chain_type: str = "stuff",
    verbose: Optional[bool] = None,
    callback_manager: Optional[BaseCallbackManager] = None,
    **kwargs: Any,
) -> BaseCombineDocumentsChain:
    """Load question answering chain.

    Args:
        llm: Language Model to use in the chain.
        chain_type: Type of document combining chain to use. Should be one of "stuff",
            "map_reduce", "map_rerank", and "refine".
        verbose: Whether chains should be run in verbose mode or not. Note that this
            applies to all chains that make up the final chain.
        callback_manager: Callback manager to use for the chain.

    Returns:
        A chain to use for question answering.
    """
    loader_mapping: Mapping[str, LoadingCallable] = {
        "stuff": _load_stuff_chain,
        "map_reduce": _load_map_reduce_chain,
        "refine": _load_refine_chain,
        "map_rerank": _load_map_rerank_chain,
    }
```

#### StuffDocumentsChain
è·å–ä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ï¼Œå¸¦å…¥æç¤ºä¸Šä¸‹æ–‡ï¼Œä¼ é€’ç»™LLMï¼ˆé€‚åˆå°æ–‡æ¡£ï¼‰
```python
def _load_stuff_chain(
    llm: BaseLanguageModel,
    prompt: Optional[BasePromptTemplate] = None,
    document_variable_name: str = "context",
    verbose: Optional[bool] = None,
    callback_manager: Optional[BaseCallbackManager] = None,
    callbacks: Callbacks = None,
    **kwargs: Any,
) -> StuffDocumentsChain:
```

![](https://python.langchain.com/assets/images/stuff-818da4c66ee17911bc8861c089316579.jpg)

#### RefineDocumentsChain
åœ¨Studffæ–¹å¼ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¾ªç¯è¾“å…¥æ–‡æ¡£å¹¶è¿­ä»£æ›´æ–°å…¶ç­”æ¡ˆï¼Œä»¥è·å¾—æœ€å¥½çš„æœ€ç»ˆç»“æœã€‚å…·ä½“åšæ³•æ˜¯å°†æ‰€æœ‰éæ–‡æ¡£è¾“å…¥ã€å½“å‰æ–‡æ¡£å’Œæœ€æ–°çš„ä¸­é—´ç­”æ¡ˆç»„åˆä¼ é€’ç»™LLMã€‚ï¼ˆé€‚åˆLLMä¸Šä¸‹æ–‡å¤§å°ä¸èƒ½å®¹çº³çš„å°æ–‡æ¡£ï¼‰

```python
def _load_refine_chain(
    llm: BaseLanguageModel,
    question_prompt: Optional[BasePromptTemplate] = None,
    refine_prompt: Optional[BasePromptTemplate] = None,
    document_variable_name: str = "context_str",
    initial_response_name: str = "existing_answer",
    refine_llm: Optional[BaseLanguageModel] = None,
    verbose: Optional[bool] = None,
    callback_manager: Optional[BaseCallbackManager] = None,
    callbacks: Callbacks = None,
    **kwargs: Any,
) -> RefineDocumentsChain:
```

![](https://python.langchain.com/assets/images/refine-a70f30dd7ada6fe5e3fcc40dd70de037.jpg)

#### MapReduceDocumentsChain

å°†LLMé“¾åº”ç”¨äºæ¯ä¸ªå•ç‹¬çš„æ–‡æ¡£ï¼ˆMapæ­¥éª¤ï¼‰ï¼Œå°†é“¾çš„è¾“å‡ºè§†ä¸ºæ–°æ–‡æ¡£ã€‚ç„¶åï¼Œå°†æ‰€æœ‰æ–°æ–‡æ¡£ä¼ é€’ç»™å•ç‹¬çš„åˆå¹¶æ–‡æ¡£é“¾ä»¥è·å¾—å•ä¸€è¾“å‡ºï¼ˆReduceæ­¥éª¤ï¼‰ã€‚åœ¨æ‰§è¡ŒMapæ­¥éª¤å‰ä¹Ÿå¯ä»¥å¯¹æ¯ä¸ªå•ç‹¬æ–‡æ¡£è¿›è¡Œå‹ç¼©æˆ–åˆå¹¶æ˜ å°„ï¼Œä»¥ç¡®ä¿å®ƒä»¬é€‚åˆåˆå¹¶æ–‡æ¡£é“¾ï¼›å¯ä»¥å°†è¿™ä¸ªæ­¥éª¤é€’å½’æ‰§è¡Œç›´åˆ°æ»¡è¶³è¦æ±‚ã€‚ï¼ˆé€‚åˆå¤§è§„æ¨¡æ–‡æ¡£çš„æƒ…å†µï¼‰
```python
def _load_map_reduce_chain(
    llm: BaseLanguageModel,
    question_prompt: Optional[BasePromptTemplate] = None,
    combine_prompt: Optional[BasePromptTemplate] = None,
    combine_document_variable_name: str = "summaries",
    map_reduce_document_variable_name: str = "context",
    collapse_prompt: Optional[BasePromptTemplate] = None,
    reduce_llm: Optional[BaseLanguageModel] = None,
    collapse_llm: Optional[BaseLanguageModel] = None,
    verbose: Optional[bool] = None,
    callback_manager: Optional[BaseCallbackManager] = None,
    callbacks: Callbacks = None,
    **kwargs: Any,
) -> MapReduceDocumentsChain:
```

![](https://python.langchain.com/assets/images/map_reduce-c65525a871b62f5cacef431625c4d133.jpg)

#### MapRerankDocumentsChain

æ¯ä¸ªæ–‡æ¡£ä¸Šè¿è¡Œä¸€ä¸ªåˆå§‹æç¤ºï¼Œå†ç»™å¯¹åº”è¾“å‡ºç»™ä¸€ä¸ªåˆ†æ•°ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„å›ç­”ã€‚
```python
def _load_map_rerank_chain(
    llm: BaseLanguageModel,
    prompt: BasePromptTemplate = map_rerank_prompt.PROMPT,
    verbose: bool = False,
    document_variable_name: str = "context",
    rank_key: str = "score",
    answer_key: str = "answer",
    callback_manager: Optional[BaseCallbackManager] = None,
    callbacks: Callbacks = None,
    **kwargs: Any,
) -> MapRerankDocumentsChain:
```

![](https://python.langchain.com/assets/images/map_rerank-0302b59b690c680ad6099b7bfe6d9fe5.jpg)


### è·å–é¢†åŸŸçŸ¥è¯†çš„é“¾ï¼ˆä¸“é—¨ç”¨é€”chainï¼‰
APIChainä½¿å¾—å¯ä»¥ä½¿ç”¨LLMsä¸APIè¿›è¡Œäº¤äº’ï¼Œä»¥æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚é€šè¿‡æä¾›ä¸æ‰€æä¾›çš„APIæ–‡æ¡£ç›¸å…³çš„é—®é¢˜æ¥æ„å»ºé“¾ã€‚
ä¸‹é¢æ˜¯ä¸æ’­å®¢æŸ¥è¯¢ç›¸å…³çš„

```python
import os
from langchain.llms import OpenAI
from langchain.chains.api import podcast_docs
from langchain.chains import APIChain

listen_api_key = 'xxx'
llm = OpenAI(temperature=0)
headers = {"X-ListenAPI-Key": listen_api_key}
chain = APIChain.from_llm_and_api_docs(llm, podcast_docs.PODCAST_DOCS, headers=headers, verbose=True)
chain.run("æœç´¢å…³äºChatGPTçš„èŠ‚ç›®, è¦æ±‚è¶…è¿‡30åˆ†é’Ÿï¼Œåªè¿”å›ä¸€æ¡")
```


**åˆå¹¶æ–‡æ¡£çš„é“¾çš„é«˜é¢‘ä½¿ç”¨åœºæ™¯ä¸¾ä¾‹**

### å¯¹è¯åœºæ™¯ï¼ˆæœ€å¹¿æ³›ï¼‰
ConversationalRetrievalChain å¯¹è¯å¼æ£€ç´¢é“¾çš„å·¥ä½œåŸç†ï¼šå°†èŠå¤©å†å²è®°å½•ï¼ˆæ˜¾å¼ä¼ å…¥æˆ–ä»æä¾›çš„å†…å­˜ä¸­æ£€ç´¢ï¼‰å’Œé—®é¢˜åˆå¹¶åˆ°ä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ä¸­ï¼Œç„¶åä»æ£€ç´¢å™¨æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£ï¼Œæœ€åå°†è¿™äº›æ–‡æ¡£å’Œé—®é¢˜ä¼ é€’ç»™é—®ç­”é“¾ä»¥è¿”å›å“åº”ã€‚
```python
def test_converstion():
    from langchain.chains import ConversationalRetrievalChain
    from langchain.memory import ConversationBufferMemory
    loader = TextLoader("./test.txt")
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    documents = text_splitter.split_documents(documents)

    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(documents, embeddings)
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), memory=memory)
    query = "è¿™æœ¬ä¹¦åŒ…å«å“ªäº›å†…å®¹ï¼Ÿ"
    result = qa({"question": query})
    print(result)
    chat_history = [(query, result["answer"])]
    query = "è¿˜æœ‰è¦è¡¥å……çš„å—"
    result = qa({"question": query, "chat_history": chat_history})
    print(result["answer"])
```

### åŸºäºæ•°æ®åº“é—®ç­”åœºæ™¯
```python
def test_db_chain():
    from langchain import OpenAI, SQLDatabase, SQLDatabaseChain
    db = SQLDatabase.from_uri("sqlite:///../user.db")
    llm = OpenAI(temperature=0, verbose=True)
    db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True)
    db_chain.run("æœ‰å¤šå°‘ç”¨æˆ·?")
```

### æ€»ç»“åœºæ™¯
```python
def test_summary():
    from langchain.chains.summarize import load_summarize_chain
    
    text_splitter = CharacterTextSplitter()
    with open("./æµ‹è¯•.txt") as f:
        state_of_the_union = f.read()
    texts = text_splitter.split_text(state_of_the_union)
    docs = [Document(page_content=t) for t in texts[:3]]
    chain = load_summarize_chain(OpenAI(temperature=0), chain_type="map_reduce")
    chain.run(docs)
```

### é—®ç­”åœºæ™¯
```python
def test_qa():
    from langchain.chains.question_answering import load_qa_chain
    loader = TextLoader("./æµ‹è¯•.txt")
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()
    docsearch = Chroma.from_documents(texts, embeddings)
    qa_chain = load_qa_chain(OpenAI(temperature=0), chain_type="map_reduce")
    qa = RetrievalQA(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever())
    qa.run()
```